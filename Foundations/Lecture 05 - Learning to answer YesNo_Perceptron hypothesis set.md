# Perceptron Hypothesis Set
## Perceptron
<br> Continue example of credit card, we will use the perceptron to give out an algorithm.<br/>
<br>![image](https://github.com/yhlien1221/Machine_Learning_Foundations_and_Techniques/blob/main/Foundations/pic/5_1.jpg)<br/>
* Customers have different features, like salary or debt and that is a vector in x=(x1,x2....).
* Then we give weights to each feature, like w1, w2.... (also a vector)
* Then we do the inner product of features and weights, then we got score from each customer. We use the score to minus threshold then got output. 
* Approve credit if weighted score > threshold, deny credit if weighted score < threshold
* We can get a different hypothesis by changing threshold or weight. The hypothesis in this case is called __Perceptron__. 
## Threshold can be a part of vector by simplifying the formula (add w0,x0)
<br>![image](https://github.com/yhlien1221/Machine_Learning_Foundations_and_Techniques/blob/main/Foundations/pic/5_2.jpg)<br/>
* What do perceptron *h* "look like"?
## What do perceptron *h* "look like"?
<br>![image](https://github.com/yhlien1221/Machine_Learning_Foundations_and_Techniques/blob/main/Foundations/pic/5_3.jpg)<br/>
*Assume we have some features in a R2(二維) vector, then h(x) is a line. One side of the line is positive and another side is negative. It can separate data into credit (circle) or deny credit (X), so we can understand that __Perceptron__ is a linear (binary) classifiers.
## Perceptron Learning algorithm (PLA)
* Find a perceptron
<br>![image](https://github.com/yhlien1221/Machine_Learning_Foundations_and_Techniques/blob/main/Foundations/pic/5_3_1.jpg)<br/>
<br>How do we find a best hypothesis? We want to know f(x) but we usally don't know it. We can find a "g" that may approximate "f", then we put Data(D) into g, if the y that generates by g matched the y in Data(D), so we can say "g" is close to "f" (or the same).<br/>
<br>![image](https://github.com/yhlien1221/Machine_Learning_Foundations_and_Techniques/blob/main/Foundations/pic/5_4.jpg)<br/>
* start from w0, and "correct" its mistakes on D
* Groud truth y=+1 but we got y=-1, which means we should minimize the included angle between w and x, we can modify w into w+y.
* Groud truth y=-1 but we got y=+1, which means we should enlarge the included angle between w and x, we can modify w into w+y.
* 以內積來說，兩向量__夾角變小__，其__值會變大__； __夾角變大__ ，其__值會變小__ 
## Cyclic PLA
* How do we make sure the PLA algorithm is great?
> We can put each data into g that generated by data, then check if the y generated by g matched the y in data until a full cycle of not encountering mistakes.
## The prococedure of how to find a best perceptron
<br>![image](https://github.com/yhlien1221/Machine_Learning_Foundations_and_Techniques/blob/main/Foundations/pic/5_5.jpg)<br/>
* There is no line at the beginning.
<br>![image](https://github.com/yhlien1221/Machine_Learning_Foundations_and_Techniques/blob/main/Foundations/pic/5_6.jpg)<br/>
* Update 1:Start from a central point (origin), then we find x1 and add a line (法向量, normal vector) to origin . 
<br>![image](https://github.com/yhlien1221/Machine_Learning_Foundations_and_Techniques/blob/main/Foundations/pic/5_7.jpg)<br/>
* Update 2:A black circle is in the wrong side. Original we should change the normal vector from red line to purple one by minimizing the included angle.
<br>![image](https://github.com/yhlien1221/Machine_Learning_Foundations_and_Techniques/blob/main/Foundations/pic/5_8.jpg)<br/>
* Update 3:A black X is wrong. We should change the normal vector from red line to purple one by enlarging the included angle.
<br>![image](https://github.com/yhlien1221/Machine_Learning_Foundations_and_Techniques/blob/main/Foundations/pic/5_9.jpg)<br/>
* Update 4: continue updated the line.
<br>![image](https://github.com/yhlien1221/Machine_Learning_Foundations_and_Techniques/blob/main/Foundations/pic/5_10.jpg)<br/>




<!-- ref
http://naivered.github.io/2016/07/05/Study_Notes/Machine%20Learning%20Foundations/Machine-Learning-Foundations-L2-Notes-1/

-->
